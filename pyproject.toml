[project]
name = "freeze-attn"
version = "0.1.0"
description = "Freeze attention in Llama models"
authors = [{ name = "Donald Thompson", email = "witt3rd@witt3rd.com" }]
dependencies = [
    "tiktoken>=0.8.0",
    "blobfile>=3.0.0",
    "torch>=2.5.1",
    "transformers>=4.47.1",
    "accelerate>=1.2.1",
    "sentencepiece>=0.2.0",
    "protobuf>=5.29.2",
    "huggingface-hub>=0.27.1",
]
readme = "README.md"
requires-python = ">= 3.12"

[tool.pyright]
include = ["src"]
extraPaths = ["typings"]
typeCheckingMode = "basic"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.rye]
managed = true
dev-dependencies = ["ruff>=0.8.6", "basedpyright>=1.23.2", "pre-commit>=4.0.1"]

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["src/llama"]

[tool.rye.scripts]
format = "ruff format src/"
lint = "ruff check src/"
typecheck = "basedpyright src/"
validate = { chain = ["format", "lint", "typecheck"] }
pc = { cmd = "pre-commit run --all-files", cwd = "src/" }
